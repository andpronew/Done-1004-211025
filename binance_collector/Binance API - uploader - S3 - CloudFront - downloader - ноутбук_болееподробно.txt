Binance API - uploader - S3 - CloudFront - downloader - ноутбук: масимально подробно:

Когда файлы появляются в ./local_binance:
В нашей программе uploader цикл устроен так (настроечное поле sleep_seconds, по умолчанию 120 s):
- В начале итерации uploader делает HTTP-запрос к Binance (публичному /api/v3/aggTrades или приватному /api/v3/myTrades, в зависимости от use_signed_api).
- Ответ от Binance (JSON) сразу записывается в локальный файл ./local_binance/bn_<SYMBOL>_<TIMESTAMP>.json.
- Затем этот файл сжимается командой gzip, получается ...json.gz.
- После успешного сжатия uploader загружает ...json.gz в S3 (aws s3 cp ... s3://<bucket>/data/<file>).
- После успешной загрузки uploader обновляет manifest (создаёт/скачивает временный manifest, вставляет новую запись сверху, загружает tmp → s3://.../data/manifest.json с Cache-Control: no-cache...).
- Только после успешной загрузки файла в S3 и обновления manifest локальный файл ...json.gz удаляется (чтобы не заполнять диск).
- Затем uploader делает sleep(sleep_seconds) и цикл повторяется.

Итог: файл появляется локально мгновенно в момент завершения запроса к Binance, и остаётся локально до тех пор, пока не будет успешно загружен в S3 и manifest не будет обновлён — после этого локальный буфер очищается.
Если upload/manifest-update упал (ошибка сети, права и т.п.), локальный файл не удаляется — его можно retry'ить в следующей итерации. Это важная гарантия: данные не теряются из-за transient ошибок.

Полная цепочка и что именно происходит в каждой точке (подробный разбор шагов, сетевых вызовов, заголовков, прав и возможных ошибок):
1) Источник: Binance API (public или signed)
uploader делает HTTP GET:
Public: GET https://api.binance.com/api/v3/aggTrades?symbol=BTCUSDT&limit=1000
Signed (private): GET https://api.binance.com/api/v3/myTrades?symbol=BTCUSDT&timestamp=<ms>&recvWindow=5000&signature=<hex> + header X-MBX-APIKEY: <api_key>
Ответ — JSON, часто большой (несколько KB…MB в зависимости от лимита).
Если запрос неудачный (HTTP ≥400 или сетевой сбой), uploader логирует ошибку, ждёт и повторит (в коде — просто ждёт след. итерацию; можно добавить экспоненциальный backoff).

2) Локально: ./local_binance
Ответ пишется в файл bn_<symbol>_<timestamp>.json.
Сразу gzip'ится (gzip -f), т.е. получаем bn_...json.gz.
Локальная папка — буфер: временное хранилище пока файл не ушёл в S3. В коде мы удаляем локальный .gz только после удачной загрузки в S3.
Если диск заполнится (маловероятно при удалении после загрузки), uploader начнёт жаловаться на ошибки записи; можно добавить проверку свободного места перед fetch.

3) Загрузка в S3 (aws s3 cp)
Команда: aws s3 cp localfile s3://bucket/data/<name> --cache-control "max-age=31536000" --acl private
Важные пункты:
--acl private + CloudFront OAI: объекты приватные, CloudFront получает доступ через Origin Access Identity.
Cache-Control: max-age=31536000 — долгий TTL для больших статичных файлов (CloudFront будет кэшировать их на edge).
S3 гарантирует strong read-after-write consistency: сразу после успешного PUT можно читать объект (с ноября 2020 S3 поддерживает strong consistency).
Если aws s3 cp вернул ошибку — локалка сохраняется (повторная попытка в следующей итерации).

4) Обновление manifest.json (атомарно)
downloader-style: uploader качает текущий manifest (если есть) во временный файл /tmp/manifest.tmp.json или создаёт [].
Добавляет новую запись в начало массива (newest-first), обрезает до max_entries.
Сохраняет tmp и делает aws s3 cp tmp s3://bucket/data/manifest.json --cache-control "no-cache, no-store, must-revalidate, max-age=0" --acl private
Почему так: manifest — «навигационная карта» для downloader. Мы сначала загружаем файл—а только потом удаляем локальный файл, чтобы избежать гонок: downloader должен видеть только те записи, для которых уже есть соответствующие объекты в S3.
Cache-Control для manifest = no-cache → CloudFront при получении manifest будет каждый раз проверять у origin (или с TTL=0 поведение) и выдавать свежую версию, минимизируя проблему кэширования.

5) S3 → CloudFront (origin pull)
CloudFront настроен с Origin = ваш S3 bucket и Origin Access Identity (OAI) → S3 объекты остаются приватными.
Когда клиент (downloader) запрашивает https://<CF_DOMAIN>/data/manifest.json:
Edge node CloudFront смотрит в кэш. Если нет или требуется revalidate (manifest no-cache), CloudFront делает запрос к S3 origin (от лица OAI) и получает актуальный manifest.
Для больших .gz файлов: при первом запросе edge node делает origin fetch (S3 → CloudFront), и затем кэширует объект в edge (TTL = файлу задан via Cache-Control или CloudFront DefaultTTL).
Важное: S3 → CloudFront origin fetch внутри AWS обычно не тарифицируется как egress (S3 → CloudFront считается внутренним трафиком и безопасен). Egress оплачивается только при выходе из CloudFront к интернет-пользователям (но первые 1 TB/мес CloudFront egress — бесплатно).

6) Downloader: чтение manifest и загрузка
Downloader делает curl https://<CF_DOMAIN>/data/manifest.json.
Благодаря Cache-Control: no-cache или CloudFront behaviour TTL=0, manifest быстро обновляется для клиента.
Downloader парсит JSON (newest-first), но итерация идёт в обратном порядке (oldest → newest), чтобы скачивать файлы в хронологическом порядке (это было реализовано сознательно).
Для каждой записи:
Если имя уже есть в state_file (локальная база скачанных) → пропускает.
Иначе делает curl -f -L -C - -o <local> для URL https://<CF_DOMAIN>/<key>; -C - поддерживает дозагрузку (resume), а -f — чтобы curl возвращал ошибку при HTTP>=400.
По успешной загрузке добавляет имя в state_file.
Если CloudFront вернул 404 (напр., объект удалён lifecycle'ом до того, как downloader успел), downloader логирует ошибку и продолжает; можно добавить логику пометки такого файла как «удалён» и удалять запись из manifest/состояния (вариант).

7) Эффект на биллинг и кеширование
Вы платите:
S3 хранение ($/GB/мес).
S3 PUT/GET операции (небольшие суммы).
CloudFront: первые 1 TB egress/мес — бесплатно; далее $/GB в зависимости от страны/объёма. Запросы у CloudFront тоже стоят, но очень дёшево.
Преимущество: вы избегаете дорогостоящего egress из региона EC2 (например, Tokyo $162/TB) — теперь трафик идёт из CloudFront к вам (покрывается free 1TB) и origin fetch'ы не тарифицируются как интернет egress.

Более подробный анализ расходов:
Допущения:
Файлы создаются каждые 2 минуты → 30 файлов/час → 720 файлов/сутки → ≈21 600 файлов/мес
Объём данных в месяц: от 0.5 TB (≈512 GB) до 1 TB (≈1024 GB)
Для каждого нового пакета: uploader делает PUT объекта (загрузка .json.gz) и затем PUT manifest.json (обновление manifest). Т.е. примерно 2 PUT-запроса на один файл (упрощённое, но реалистичное).

1) Сколько PUT-запросов в месяц? (количество)
Файлов в мес ≈ 21 600.
PUT на файл (объект) = 21 600.
PUT на manifest (по одному обновлению на файл) = 21 600.
Итого PUT-запросов ≈ 43 200 / месяц.
Примечание (multipart): если вы загружаете очень большие файлы, AWS CLI/SDK может использовать multipart upload — каждая часть создаёт дополнительные API-вызовы. Для типичных маленьких/средних файлов (несколько MB — десятки MB) это ≈1 PUT/объект; для файлов >>100 MB части увеличат число запросов.

2) Сколько это стоит (PUT request costs)?
Тариф для PUT/COPY/POST/LIST примерно $0.005 за 1 000 запросов (регионально может чуть отличаться). 
Стоимость для 43 200 PUT: 43 200 / 1000 = 43.2 единицы × $0.005 = $0.216 / месяц.

Можно также выразить стоимость одного PUT: $0.005 / 1000 = $0.000005 за PUT.
Для 21 600 (только сами объекты) = 21.6 × $0.005 = $0.108; монтаж manifest даёт ещё ≈$0.108. Совокупно ≈ $0.216.

3) GET / другие запросы (примерно)
Ваш downloader скачивает файлы через CloudFront (edge). Если CloudFront кэширует данные — GET запросы к S3 минимальны (origin fetch только при cache-miss или первой загрузке).
Если же Downloader бьёт прямо в S3 (или CloudFront постоянно делает origin fetch за manifest), то GET-запросов может быть порядка 21 600 (скачиваний файлов) + 21 600 (manifest pulls) = 43 200 GETs. Тариф GET ≈ $0.0004 / 1 000 → 43.2 × $0.0004 = $0.0173 (очень мало). 
nOps
Практически: при использовании CloudFront большую часть трафика и запросов забирает CDN — поэтому S3 GET-чек обычно почти нулевой.

4) Хранение: сколько $/GB/мес?
Цена S3 Standard (первые 50 TB) ≈ $0.023 / GB / месяц (вариации по регионам; иногда $0.023–$0.0265 в отдельных таблицах — но $0.023 типичное значение для us-east-1). 
Рассчёт:
Объём			GB	Стоимость
0.5 TB (≈512 GB)	512	512 × $0.023 = $11.78 / мес
1.0 TB (≈1024 GB)	1024	1024 × $0.023 = $23.55 / мес
5) Итого (ориентир на месяц)
PUT-операции ≈ $0.22 / мес (по нашим допущениям)
Хранение S3: $11.8 / мес (0.5 TB) — $23.6 / мес (1 TB). 
GET/прочие — обычно пренебрежимо малы (копейки), особенно при CloudFront.
Итого (пример):
при 0.5 TB → ≈ $12.0 / мес (хранение + PUTs + мелочь)
при 1.0 TB → ≈ $23.8 / мес
Эти цифры не включают: CloudFront egress (первые 1 TB/мес бесплатны — экономия), CloudFront запросы (очень дешёво), S3-операции особых видов (переходы lifecycle в другие классы платные), а также возможные multipart-overhead и region-вариации.

6) Важные уточнения и подводные камни
Multipart uploads: если файлы большие (>100 MB), загрузка будет разбиваться на части — каждая часть считается отдельным API вызовом (и может увеличить счёт PUT-style запросов). Проверьте средний размер ваших пакетов; если вы генерируете много маленьких файлов (несколько MB), multipart — маловероятен.
DELETE: удаление объектов (DeleteObject) обычно бесплатно; но операции LIST/DELETE в групповой очистке могут приводить к LIST-запросам, которые тарифицируются как PUT/COPY/POST/LIST ($0.005/1000). (AWS doc: DELETE и CANCEL — free). 
Lifecycle transitions (если вы переводите объекты в Glacier/IA) могут стоить дополнительно (запросы перехода). Если вы используете только удаление через lifecycle — стоимость удаления как таковая обычно не тарифицируется, но переходы между классами стоят. 
Регион: цены немного варьируются по регионам (Tokyo / ap-northeast-1 может стоить чуть дороже). Приведённые $0.023/GB и $0.005/1k запросов — типичные для us-east-1; проверяйте ваш регион в AWS S3 Pricing. 

7) Как уменьшить расходы (практические советы)
Уменьшить число PUT'ов:
Батчить файлы: вместо файла каждые 2 минуты — объединять, например, по 10 файлов в один архив (1 PUT вместо 10) → уменьшение PUT-стоимости в 10× (хранение тот же).
Редуцировать частоту manifest update: обновлять manifest раз в N файлов (например, каждую 10-ю загрузку) — снизит PUT на manifest. (тогда нужна синхронизация downloader'а).
Размер файлов: если файлы очень маленькие, объединение в larger archives уменьшит multipart и overhead.
Хранение: если вам нужен только краткий буфер (сутки), активируйте lifecycle → delete 1d (как сейчас). Если хотите хранить долгосрочно, рассмотрите S3-IA или Glacier Instant Retrieval (дешевле, но с retrieval fees). 
CloudFront: оставьте скачивание через CloudFront — первые 1 TB/мес egress бесплатны; это ваша основная экономия по сравнению с «прямым egress» из EC2 (тк вы раньше платили ~ $162/TB). (Если хотите, могу привести ссылки на CloudFront pricing).

8) Что лучше сделть прямо сейчас
Проверить средний размер файла. Если средний размер < 10–20 MB — подумать об агрегировании (каждые N файлов в один .tar.gz).
Сделать точный расчёт под ваш месяц: подставить фактическое число файлов/месяцев и средний размер файла

Поведение при ошибках и пограничные случаи:
Upload упал: локальный .json.gz остаётся — uploader попытается снова при следующей итерации. Можно добавить экспоненциальную задержку при частых ошибках.
Manifest обновлён, но CloudFront отдаёт старое: если manifest имеет Cache-Control: no-cache, edge node сделает conditional GET; иногда может быть небольшая задержка, поэтому можно:
- поставить CloudFront behavior для data/manifest.json с TTL=0, либо
- посылать CloudFront Invalidation для manifest (но инвалидации платные после бесплатных 1000 путей).
Downloader скачал файл, но checksum не соответствует: можно проверять sha256 из manifest (если uploader добавлял это поле). При несоответствии — удалить файл и перекачать/логировать.
Lifecycle удалил объект раньше, чем downloader успел: downloader получит 404; логировать и, возможно, пометить как «пропущено». Желательно, когда lifecycle удаляет объекты, manifest тоже очищается (uploader/cron мог бы это делать), либо manifest-entries стареют и downloader игнорирует записи старше N ms.
Сбой CloudFront дистрибутива: возможны ошибки (5xx); тогда downloader получает ошибки — добавить retry с backoff.

Рекомендации по улучшению/надёжности:
Ретрай и backoff для сетевых операций (Binance fetch, aws s3 cp, curl download).
Проверка целостности: вычислять sha256 перед загрузкой и добавлять в manifest; downloader проверяет после загрузки.

Мониторинг:
CloudWatch для CloudFront (bytes, requests, 4xx/5xx).
Billing Alarm на использование CloudFront egress > 1 TB.
Логи uploader/downloader через systemd или файлы логов.

Безопасность:
использовать OAI, чтобы S3 объекты были приватными;
опционально — CloudFront Signed URLs, если хотите выдавать временный доступ третьим сторонам;
шифрование на S3: SSE-S3 или SSE-KMS (KMS даёт дополнительную защиту, но может стоить дороже).
Версионирование S3 — если хотите возможность отката/восстановления, включите versioning.
Параллельная загрузка/скачивание — для ускорения, особенно если файлы большие; downloader можно сделать многопоточным.
Graceful shutdown и атомарные операции (tmp files) — уже учтено для manifest; можно так же писать локальные tmp файлы и переименовывать.
Как вручную проверить/диагностировать (полезные команды)

Краткая блок-схема:
Fetch: uploader → Binance API → local bn_...json → gzip → local bn_...json.gz
Upload: local .gz → aws s3 cp → s3://bucket/data/<file>
Manifest: updater загружает tmp manifest → s3://bucket/data/manifest.json (no-cache)
Distribution: CloudFront (edge) запрашивает manifest (revalidate у origin) → отдаёт клиенту
Download: downloader читает manifest → по списку скачивает https://<CF_DOMAIN>/<key> → сохраняет локально и помечает в state_file
Cleanup: S3 lifecycle удаляет старые объекты (>1d), uploader локально удаляет только после успешного upload+manifest
